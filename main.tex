\documentclass[12pt, UTF8]{ctexart}
\CTEXsetup[format={\Large\bfseries}]{section}  
\usepackage[colorlinks,linkcolor=red]{hyperref}
\usepackage{graphicx,amssymb,bm}
\pagestyle{plain}
\begin{document}
	\section*{摘要}
		我们建议使用降维来防御针对ML分类器的规避攻击。我们研究了一种通过主成分分析来降维的策略，以增强机器学习的适应能力，既可以应用于分类又可应用于训练阶段。我们使用多个真实世界的数据集证明了数据降维在防御逃避攻击方面的可行性。我们的主要研究结果是：（1）有效对抗文献中的策略性的逃避攻击，将对方成功攻击所需的资源增加约2倍，（2）适用于一系列ML分类器 包括支持向量机和深度神经网络，（3）可推广到多个应用领域，包括图像分类和人类活动分类。
	\section{介绍}
		我们生活在一个到处充满着机器学习（ML）和人工智能的时代。机器学习被用于诸如图像识别，自然语言处理，垃圾邮件检测，车辆自动驾驶甚至恶意软件检测等多种基础应用中。
		
		此外，最近在深度学习方面取得的进展表明分类的准确性可以接近于人类操作的准确性，这使得ML系统的广泛应用成为可能。 鉴于ML应用程序的无处不在，它越来越多地应用于敌对情景中，在这种情况下，攻击者可以从ML系统的失败中对输入进行正确的分类。那么问题就出现了：ML系统在对抗环境中安全吗？
		
		对抗性机器学习：从21世纪初开始，已经有大量工作将机器学习算法的脆弱性暴露给战略对手。例如，中毒攻击在训练阶段系统地引入敌对数据，从而在测试阶段导致数据分类错误。另一方面，规避攻击的目的是通过向测试数据中添加策略性的干扰数据来欺骗现有的ML分类器。
		
		规避攻击：在本文中，我们重点关注规避攻击，其中攻击者的目标是干扰ML分类器的测试输入以引起错误分类。针对各种机器学习分类器都提出过规避攻击，如支持向量机，基于树的分类器，随机森林和增强树，以及最近的神经网络。使用机器学习的应用程序（例如人脸检测，语音命令识别和PDF恶意软件检测）的脆弱性也已得到证明，这也突出了防御的必要性。令人惊讶的是，这也表明，敌对方修改后的数据（针对特定分类器）的规避属性持续存在于不同的ML分类器中，这使得即使对ML系统了解很有限的对手都可以攻击它。因此，在敌对情境下使用ML系统时考虑敌对数据和躲避攻击的可能性至关重要。然而，针对这些攻击的防御措施极少，并且每种攻击的适用性仅限于某些已知的攻击和特定类型的ML分类器（请参见第7节获得详细描述）。
	\subsection{贡献}
		通过广泛的评估，我们发现我们的防御机制明显降低了逃避攻击的成功率。就我们所知，这是针对具有以下属性的规避攻击的唯一防御措施：（1）适用于多个ML分类器（如SVM，DNN），（2）适用于多个应用领域（图像和活动分类），（3）减轻多种攻击类型，包括战略攻击类型。此外，我们的防御可调性允许系统设计人员根据应用选择公共安防权衡曲线上适当的操作点。
	\subsubsection{防御}
		在本文中，我们提出使用数据的降维来防御针对ML系统的规避攻击。降维技术（如主成分分析）旨在将高维数据投影到较低维度的空间，同时满足特定的条件。我们研究了一种降维的策略，以增强机器学习的适应能力，既可以应用于分类又可应用于训练阶段。我们考虑一种方法，将降维应用于训练数据和测试数据，以增强训练分类器的可靠性。
	\subsubsection{实证评估}
		我们证明了我们的防御措施的可行性和有效性：
		\begin{itemize}
			\item 多重分类器，例如支持向量机（SVM）和深度神经网络（DNN）
			\item 几种不同类型的规避攻击，例如Moosavi-Dezfooli等人对线性SVMs的攻击、Goodfel-low等人的深层神经网络攻击以及针对我们的防御措施的策略性攻击
			\item 各种现实世界的数据集/应用程序：MNIST图像数据集和UCI人类活动识别（HAR）数据集。
		\end{itemize}	
	
		我们的主要发现是，即使面对一个几乎完全了解ML系统的强大对手，（1）我们的防御措施使得成功攻击所需的修改程度有着高达5倍的显著提高，同样的，以固定的修改程度攻击的成功率降低约2-50倍，（2）防御措施可以用于不同的ML分类器，对原始分类器进行最小限度的修改，
		同时仍然有效地防御攻击，（3）在大多数情况下良性样品的分类成功率有约1-4％的适度变化。我们还提供了公共安防权衡曲线的分析以及我们的防御措施产生的计算开销。我们的结果开源在\url{https: //github.com/inspire-group/ml_defense}上 。

		然而，我们的防御措施并没有完全解决规避攻击的问题，因为它可以降低固定预算下的敌对成功率，但这并不是在所有情况下都忽略不计。在第4节中，我们讨论了对手在不同应用场景下可用的预算范围，并明确了防御有效的场景。我们希望我们的工作能够激发进一步的研究，以解决规避攻击来保证机器学习的系统的安全性。
		
		本文的其余部分安排如下：首先，在第2节中，我们介绍了对抗机器学习的必要背景。然后，在第3节中，我们描述了我们的防守措施。接下来，我们分别在第4节和第5节中设置并提出我们的实证评估。我们在第6节讨论我们的结果。最后，我们在第7节中详细介绍相关工作，并在第8节中做出结论。
		
	\section{对抗性机器学习}
		在本节中，我们提出了对抗性机器学习所需的背景，重点关注（a）ML分类器，如SVM和DNN，以及（b）通过干扰测试输入引发错误分类的规避攻击。
		
		动机和运行示例：我们的运行示例使用来自MNIST数据集的图像数据（详见第4节）。图1（a）描绘了来自MNIST数据集的正确测试图像，这些图像被SVM分类器正确分类；而图1（b）描绘了对手制作的测试图像（使用Papernot的规避攻击的扰动图像），它们被SVM分类器错误分类。
		
		\includegraphics[width = .9\textwidth]{figure_1.png}
	\subsection{使用机器学习分类}
		在本文中，我们关注有监督的机器学习，其中分类器通过预先存在的标签对数据进行训练。一个训练完成的监督机器学习分类器是一个函数，通过输入点$\bm{x}\in \mathbb{R}^d$（二进制时为$\{0,1\}d$），会输出$\hat{y}\in C$，其中C是所有可能分类的集合。例如，在MNIST数据集的情况下，$\bm{x}$将是28×28像素的手写数字的灰度图像，而C将是有限集合\{0,1,2,3,4,5,6,7,8,9\}。
	\subsection{攻击机器学习系统}
		在本小节中，我们首先讨论对抗模型，之后我们会讲述一般的规避攻击，最后讲述对特定的ML分类器的规避攻击。
		
		注：我们把完整的训练集表示为$S_{train}$，完整的训练数据表示为$S_{test}$，将ML分类器表示为$f$，并且针对ML分类器的特定参数表示为$\bm{\theta}$。数据的原始维度表示为$d$。接下来，我们把攻击者的攻击算法表示为$A(\bm{x}_{in}|K)$，其中，$\bm{x}_{in}$表示对手开始时的输入，$K$代表对手的已知信息，可能是$\{S_{train},f,\bm{\theta}\}$的任一子集。$\bm{\widetilde{x}}$表示A生成的敌对样本。
	\subsubsection{敌对方的目标和能力}
		在本文中，我们关注的情景是，攻击者的目标是通过修改一个正确的输入，以便使它被误分为其他的任何分类，或者使其被归类为与原始类不同的目标分类。请注意，这些目标分类在二元分类器的情况下是等价的。
		
		我们的基本假设是对手具有以下能力。
		\begin{itemize}
			\item 对手完全了解原始分类器已经训练过的训练集，即她知道分类器作为输入所采用的特征向量的类型。
			\item 对手知道分类器结构，超参数和训练过程。
			\item 错误数据是由对手离线创建的，在测试阶段提交给ML分类器。
		\end{itemize}	
		
		简而言之，$\bm{\widetilde{x}}=A(\bm{x}_{in}|S_{train},f,\bm{\theta},K_{add})$，其中$K_{add}$表示关于对手可能拥有的系统的任何其他知识。
		
		我们对对手的能力的假设是保守的，因为从安全角度来讲，系统在完全了解系统安全的对手的强力的攻击下，依旧是健壮的。而且，一个有着ML系统知识的攻击者，即使有着有限权限的访问（如黑盒访问），也可以很好的对分类器进行推断来进行规避攻击。这和一个拥有完全访问权的对手攻击的效果集合一样，这证明了我们的假设是合理的。
	\subsubsection{规避攻击}
		在正常操作，即没有攻击者时，当输入$\bm{x}_i\in S$，$f$会输出$\hat{y}$，其中$S$是输入集合。输出的分类中正确匹配的比例为$\alpha$，即，
		\begin{equation}
			\alpha(S)=\frac{\#\{(\bm{x},y)\in S:f(\bm{x})=y\}}{\#S}
		\end{equation}
		其中$＃$给出了一组的基数。
		攻击者的目标是设计一个作用在$x\in S$上的算法A来生成敌对数据，即，$A(\bm{x})=\bm{\widetilde{x}}$，令
		$$S^{adv}=\{(A(\bm{x}),y):(\bm{x},y)\in S\}$$这是一组对比修改的例子，其中修改之处应满足：
		\begin{itemize}
			\item 与分类器的正常操作相比，增加错误分类的占比，即$\alpha(S^{adv})\textless \alpha(S)$，
			\item 在诸如图像和文本等人类可解释的数据的情况下，不被人类察觉到异常；在诸如恶意软件样本，网络和系统日志等数据的情况下，可被基于规则的检测系统通过。例如：在恶意软件的情况下，攻击者受到这样的限制，即她的修改必须确保最终的样本仍然是恶意的。
			
			我们接下来讨论敌对干扰，以及在图片数据的情况下，他们对人类的感知力。
		\end{itemize}
	\subsubsection{敌对干扰}
		模拟人类对图像扰动的感知是一个难题。作为人类可感知性的代理，我们将对某个范数$||\cdot||$的修正程度定义为$||A(\bm{x})-\bm{x}||$。需要强调的是，我们将考虑受$\ell_2$约束的限制，即$||\bm{\widetilde{x}}-\bm{x}||_2\leqslant \xi$，其中$\xi$决定了干扰的强度。[35]中给出了用于约束敌对干扰的各种规范与其感知之间关系的详细描述。
		
		现在，我们定义了实现不同对抗目标所需的最小扰动。为了在特定的类$z$中导致错误分类，必须添加一个输入数据$(\bm{x},y)$作为最小的扰动，其中$z\not= y$，
		$$\Delta(\bm{x},z)=\inf_{\bm{\widetilde{x}}}\{||\bm{\widetilde{x}}-\bm{x}||:f(\bm{\widetilde{x}}=z)\}$$
		这是导致$\bm{x}$被归类为$z$所需的最小失真。导致$\bm{x}$在任何类中被错误分类所需的最小失真是，
		$$\Delta(x)=\min_{z\in C\backslash\{y\}}\Delta(\bm{x},z)$$
		
		对于图像数据，这些量与最小可检测失真之间的关系决定了分类器$f$对敌对扰动的鲁棒性。在图1中，图像中的干扰值导致线性SVM几乎将所有输入都错误错误，但干扰对于人眼几乎不可见。这表明线性标准形式的SVM对抗扰动是不稳健的。在第4.4节中进一步讨论了用于约束对手的指标。
	\subsection{针对特定分类器的规避攻击}
		我们现在描述现有文献记载的针对特定ML分类器的攻击，并展示来自MNIST数据集的一些对抗性例子。 表1给出了各种攻击的总结。
	\subsubsection{对线性SVM的最佳攻击}
		在线性支持向量机的多类分类设置中，分类器$g_i$针对每个类别$i\in C$进行训练，其中
		\begin{equation}
			g_i:\bm{x}\mapsto \bm{w}^{T}_{i}\bm{x}+b_i
		\end{equation}
		$\bm{x}$被分配给类$f(\bm{x})=\arg\max_{i\in C}g_{i}(\bm{x})$。假定真正的类别是$t\in C$，攻击的目标是找到最接近的点$\bm{\widetilde{x}}$，使得$f(\bm{\widetilde{x}})\not=t$。
		
		从[29]我们知道，对于多类分类器的最优无目标的攻击，即如果我们只关心$f(\bm{\widetilde{x}})$使得$||\bm{\widetilde{x}}-\bm{x}||$尽可能小，令$\bm{\widetilde{x}}$的最优选择是$\bm{\widetilde{x}_k}$，则，
		\begin{equation}
			k=\mathop{\arg\min}_j\frac{g_t(\bm{x})-g_j(\bm{x})}{||\bm{w}_t-\bm{w}_j||}
		\end{equation}
		进而得到，
		\begin{equation}
			\bm{\widetilde{x}}(\xi)=\bm{x}+\xi\frac{\bm{w}_t-\bm{w}_k}{||\bm{w}_t-\bm{w}_k||}
		\end{equation}
		这里的$\xi$代表干扰的程度。导致误分类的$\xi$的最小值是$\xi^*=\frac{|g_t(\bm{x}-g_k(\bm{x}))|}{||\bm{w}_t-\bm{w}_k||}$。很容易可以证明$f(\bm{\widetilde{x}}(\xi^*))=k$。请注意，由于$||\xi\frac{\bm{w}_t-\bm{w}_k}{||\bm{w}_t-\bm{w}_k||}||=\xi$，这个攻击受到$\ell_2$约束的限制。
	\subsubsection{基于梯度的神经网络攻击}
		FGS攻击是[19]中引入的针对神经网络的高效攻击。在这种情况下，通过添加与损失函数的梯度（$\nabla J_f(\bm{x},y,\theta)$）成正比的对立噪声来生成对抗示例，其中$J_f(·)$表示损失函数，$\theta$表示用于训练的超参数。可以使用反向传播有效地计算梯度。具体为，
		\begin{equation}
			\bm{\widetilde{x}}=\bm{x}+\eta sign(\nabla J_f(\bm{x},y,\theta))
		\end{equation}
		其中$\eta$是对手可以改变的参数，以控制对抗性例子的有效性。随着$\eta$的增加，攻击的成功率一般也在增长。然而，较大的干扰可能会使人们难以辨识图像（请参阅附录中的图像，范围为h）。
		
		
	\subsection{降低机器学习的维度}
	\section{基于降维的防御}
	\section{进行实验}
	\section{实验结果}
	\subsection{防御对支持向量机的影响}
		在本节中，我们将概述我们的实验结果。我们试图回答的主要问题有：
		\begin{itemize}
			\item[i)] 我们的防御对策略性攻击是否有效？
			\item[ii)] description
			\item[iii)] description
			\item[iv)] description
		\end{itemize}
	\subsubsection{防御普通攻击}
		图4显示了防御成功防御对SVMs的防御攻击的变化。防御大大降低了敌对的成功率。例如，在e=1.0时，使用PCA的防御方法减少了k=50的维度，减少了对手的成功从99.97% 到1.85%. 这是一个98.12%或者大约54%的敌对成功率。在e=0·5的情况下，敌对的成功率是92.77% ,k=50的防御率将敌对成功率降低到0·9%这是一个103（两个数量级）下降。使用减少维度数据的训练会导致更健壮的线性SVMs，这可以从防御的附加效果中看到。
		图5：对MNIST数据集的防御有效性与对线性SVMs的最佳攻击的有效性。在MNIST数据集上的对抗示例成功了，与微扰的大小e=kx xk相比较。对每一个减少的维度分类器进行攻击，并绘制出防御的效果。
		图6：在良性测试数据和敌对性能之间的SVM分类性能之间的权衡。对抗性的健壮性是kx xk的值，它允许对手达到50%误分辨率，同样，我们也注意到当我们减少在防御的投影步骤中使用的减少的维度k时，对抗的成功减少了。k=331，对抗成功率是48·75%在e=0时，下降到5.53%当k = 100。Atk=30，对抗性的成功率下降到2·63%，适度减少到2·52% k = 10.
		在普通攻击的情况下，防御也像一个噪音移除过程，消除了敌对的干扰并留下了干净的输入数据。与战略攻击相比，我们在普通攻击中看到的增强的鲁棒性。
	\subsubsection{防御对最佳攻击的影响}
		图5显示了针对线性SVMs的最优策略攻击的防御成功的变化。这个图对应的是对手意识到维度减少防御并将样本输入到管道中它的设计是为了最有效地避开减少的维度分类器。在0·5的扰动大小，没有防御的分类器的分类率是99·04%带有k=70的缩小维度分类器的分类率很低19.75%,代表一个80.25%或者是5·01的敌对成功率。然而，由于0·5是一个小的对抗预算，即使当缩小的维度图像被投射回像素空间时，微扰也将是不可见的。在1·3的敌对预算中，开始清晰可见（见第4.4节），没有防御的分类器的错误分类率是100%，大概是77.11%对于减少了70的分类器，几乎是23%降低敌对成功率。回想一下，避开低维度分类器所需要的扰动对人眼来说更清晰可见，使这些数字变得保守。我们也可以研究我们的防御对达到一定的敌对成功率所需要的对抗预算的效果。为了达到86.6%，需要一个0·3的预算，在没有防御的情况下进行分类，而对于一个带有k=70的分类器的所需预算是1·6，也就是5·33的增加。对应的数字达到90%误分类率是0·4，没有防御，1·7代表4增加。因此，我们的防御清楚地减少了一个非常强大的对手所进行的攻击的有效程度，它完全了解防御和分类器，以及进行最优攻击的能力。
	\subsubsection{对防御的效用-安全权衡}
		图6显示了在普通和敌对条件下的性能之间的权衡。这个数据集的最佳维数显然在50到30之间，其中的扭结发生在这里。通过使用更多的维度，在分类性能方面几乎没有什么好处，而且使用更少的性能对健壮性没有任何好处。在k=50时，我们看到测试集上的分类成功率下降了91.5%没有任何防御,90.29%在防御下，因此，大约有1·2%的效用损失，以k的值，与5·9的安全增益相比，因为需要引起50%的干扰测试集的错误分类从0·16增加到0·95。有了这些结果，我们就可以得出结论，我们的防御至少在基线情况下是有效的，对于线性SVMs的普通和最优攻击都是有效的。
		现在，我们研究了我们在神经网络上的防御表现，为了证实我们关于我们的防御在机器学习分类器中的适用性的主张。
	\subsection{防御对神经网络的影响}
	\subsection{对不同数据集的适用性}
		接下来，我们通过更改所使用的数据集来修改基线配置。我们用线性SVMs作为分类器和PCA作为维度还原算法来显示结果。我们为人类活动识别数据集提供结果。
	\subsubsection{对HAR数据集的辩护}
		在图9中，显示了由于防御而导致的对抗成功的减少。在e = 1.0,对抗性的成功率从99.56% 没有防御达91.75%，k=70 和76.21% 和k=30.为了达到错误的分类率90%，需要的摄动量是0·65，没有防御，它增加到0·876，k=70，到1·26，k=30。因此，对抗性的预算增加了2个，以达到同样的敌对成功率。对效用的影响是适度的，减少了2.3% ，k =70  5·4% k = 30, 与安全获得的收益相比，这是微不足道的。
	\subsection{对效用的影响}
		表2显示了我们的防御对良性数据的分类精度的影响。关键的结论是，神经网络和线性SVMs的精度降低到k=50的程度是最多4%，此外，我们注意到，使用PCA的维数减少实际上可以提高分类精度，当k=70时，MNIST数据集的准确性从97.47% 到97.52%，然而，更有侵略性的维度减少，这将导致分类精度的急剧下降，这是意料之中的，因为用于分类的大部分信息都丢失，这些结果突出了我们在应用领域的防御的广泛适用性。很明显，我们防御的有效性并不是来自于MNIST数据集的特定结构的产物，他们的直觉对不同的数据有影响。
	\section{讨论和限制}
	\subsection{满足设计目标}
		在第3.2节中，我们列出了任何辩护都应该具备的理想目标。首先，防御应该保持较高的分类精度。从表2中可以看出，对于数据集和分类器来说，有一系列缩小的维度对分类精度影响最小，在一些特定的情况下如图10：防御MNIST数据集的有效性，反对战略上的对cnn的攻击。在MNIST数据集上的对抗示例成功了，与微扰的h=kx xk相比较。攻击是针对每个子空间分类器（从算法2中获得的）进行的，并绘制出防御的效果。证明了这一点。其次，基于PCA的防御版本增加了一个开销，它是样本n的数字的多项式，以及在测试和测试期间的维度d的数量。训练减少维度分类器所需的时间和空间最多的是在原始高维空间中的分类器，因此，我们的防御在训练和测试阶段保持高效率。我们的辩护所带来的附加安全已经在前一节的各种设置中得到了说明。从图6中可以清楚地看出，改变尺寸允许ML系统所有者在实用安全空间中导航不同的点。然而，当一个系统可能受到攻击时，我们的防御系统并不有效，这将导致我们在下面讨论的限制。
	\subsection{限制}
		尽管我们的防御在许多情况下降低了对抗性的成功率，但有两个主要的方面，它没有成为一种针对逃避攻击的全面防御机制：
		1·在自己的不足:虽然我们的防守在各种情况下都能显著降低对手的成功率，但在某些情况下，对手的成功率仍然太高。在这种情况下，我们的防御系统很可能会被合并有其他的防御措施，如对抗训练［19］和整体方法［43］以建立一个针对逃避攻击的ML系统。我们的防御有一个优势，它可以与各种各样的ML分类器一起使用，它不会干扰其他防御机制的操作。此外，正如在第四部分示范的那样，我们的辩护导致了一种具有更大视觉感知能力的“意即性”的混乱。这可能有助于防御的防御，目的是探测敌对的扰动。
		2·缺乏普遍性:在某些情况下，我们的防御能力有限。例如,在图10中,我们看到，基于PCA的防御系统几乎没有为Papernot-CNN提供安全改进（详情见第9.3条）。这一效应很可能源于这样一个事实，即CNNs已经在其卷积层中已经处于企业领域特定的知识，另外，使用PCA进行预处理的附加层不会带来任何额外的健壮性。此外，PCA可能会减少CNN的卷积层用于分类目的的本地信息的数量。解决我们防御的局限性的一个关键步骤是使用其他维度减少技术这可以将敌对的成功降低到可以忽略的水平，并与诸如CNNs这样的分类器结合在一起。在未来的工作中，我们计划探索减少维度的技术，例如自动编码器，内核PCA和各种压缩方案，以更好地理解维度减少与分类器的鲁棒性之间的关系。
	\section{相关工作}
	\section{结论}
	\section*{致谢}
	\section*{参考}
	\section{附录}
	\subsection{测量对抗成功}
		回想一下，我们在第4部分中使用了一种特殊的对抗性的成功。还有两个相关的概念可以被使用：对于每一个x，我们检查yfor（=f（x)）=f（x）是否为f（x）。这就计算出了对抗样本的总数，其中的扰动会导致由分类器为干净的样本x分配的类发生变化。可能是这样的情况，无论是干净的还是敌对的样本都没有被分配到真正的y，因为分类器在测试装置上没有百分之百的精度（也可能在训练集上），然而，也可能是添加微扰导致分类器正确地对先前错误的输入进行分类。我们可能有f（x）6=y，但是yv=y，这是不太可能但可能发生的情况。
		对于每一个x，我们检查是否yv=y。这计算了在摄动后的类不等于真正的类的敌对样本的总数。然而，这一数字还将包括那些具有对抗性的样本，而这些样本已经被错误地分类了。在这种情况下，不管攻击或防御的有效性如何，错误分类的对抗样本的百分比不能低于基线的分类器在良性测试样本上的不准确，这代表了对任何攻击的有效性的下限。
		目前还不清楚这三种统计中哪一项在之前的工作中被认为是敌对的成功。我们在实验中计算了所有3项，发现它们是相似的。
	\subsection{用于防御评估的测试数据的直觉}
		使用的数据：规避攻击通常涉及到现有样本的修改化，使它们被错误地分类。如果一种攻击可以被认为是有效的，如果它导致了对来自训练集的敌对样本的高误分类率。由于各种原因，分类器在测试集上的准确性可能不高，而将敌对修改隔离为错误分类的原因可能是有问题的。此外，由于分类器通常是经过训练的，直到它们在训练集上有非常高的准确性，他们的决策界限反映了培训数据的分布，一个涉及到最少修改训练数据的攻击是成功的。
		我们对来自测试集的反式修改样本进行了评估，主要原因是在训练集上的过度拟合可能是防御效果的一个可能的原因，因此，对防御的准确评估应该包括从测试集中制作的对抗性样本。因此，一种防御机制使一个分类器更加安全，如果从测试集中的、经过修改的样本中，管道的分类精度高于原始分类器。精确度越高，防守就越精确。
	\subsection{无线接节点网络服务}
		我们还在一个卷积神经网络［38］上进行实验我们从纸上获得的架构，而不是et al.51这个CNNs的架构如下：它有两个卷积层，每个层有32个过滤器，后面是一个最大的池层，然后是另一个两个卷积层是64的过滤器,然后是一个最大的池层。最后，我们有两个完全连接的层，每个层都有200个神经，然后是一个回归函数输出，有10个神经元（在MNIST的10个类中）隐藏层的所有神经元都是ReLUs。我们把这个网络称为“网络报纸”。它的学习速度是0.1（在过去的10年里调整到0·01），并且50个时期的动量为0.9。批大小是MNIST的500个样本，在MNIST测试数据上我们在Papernot-CNN网络得到了一个分类精度是98.91%
\end{document}